{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sharable leave_one_out.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXI2idlRkrLG",
        "colab_type": "text"
      },
      "source": [
        "# CNN for leave-one-out training splits, time and circadian estimates included\n",
        "We generated the PSG, time, and spectrogram data pickles using [this ETL/feature extraction script](https://github.com/ericcanton/sleep_classifiers_accel_ML/blob/master/source/etl.py). Our data loading, training, and evaluation scheme is described below. If you want to dive into using the code, skip to \"Preamble\". \n",
        "\n",
        "To get the data, you can add following folder to your Google Drive from Eric Canton's Google Drive. \n",
        "*    https://drive.google.com/drive/folders/1pYoXsFHhK1X3qXIOCiTrnoWKG6MTNucy?usp=sharing  \n",
        "\n",
        "This should have two folders: pickles and neural. \n",
        "1. For now, neural is empty. Trained neural networks appear here with naming format like \"123_trained_cnn.h5\" for subject number 123. \n",
        "2. pickles contains one folder per subject. We recommend leaving out 7749105 because only about an hour of data is recorded for them (watch failure) by setting <code>exclude = ['7749105']</code> to <code>data_yielder</code> in the Training section's for loops and the function calls in the Evaluation section; this is the case at time of sharing. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWZaIGNUXzrG",
        "colab_type": "text"
      },
      "source": [
        "## 0. Processed Data loading, model training, and evaluation scheme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jOCDJBiX9Hq",
        "colab_type": "text"
      },
      "source": [
        "### 0.1 Evaluation and split generation functions\n",
        "Basically, these \"data pickles\" are saved numpy arrays containing our data points stored along axis 0. To easily generate leave-one-out train/test splits, we have divided the data into separate folders for each subject in the study. in <code>evaluators.py</code> we have defined two generators, \n",
        "\n",
        "*   <code>data_yielder</code> whose <code>next()</code> method returns a tuple\n",
        "    that depends on the <code>nn</code> parameter of this Yielder. \n",
        "    If <code>nn=True</code> (the default) when called, the tuple is\n",
        "\n",
        " <code>(spectrogram[i], time[i], psg labels[i], neural network[i]) </code>  \n",
        "    enumerated according with the order returned by <code>os.walk</code> on \n",
        "    the <code>data_path</code> folder. The first three elements are numpy\n",
        "    arrays of shape (\\*, 65, 73), (\\*, 1), (\\*, 1); the last is a keras.model\n",
        "    trained on the data for all subjects except subject[i]. \n",
        "\n",
        "    If <code>nn=False</code> then returns just the first three elements above,\n",
        "\n",
        " <code>(spectrogram[i], time[i], psg labels[i]).</code>  \n",
        "\n",
        "    This default mode is intended to provide testing splits and neural networks \n",
        "    for evaluation. Specifically, the function <code>pr_roc_from_path</code>\n",
        "    was written with a <code>data_yielder</code> instance to be passed as \n",
        "    the first argument, though it functions perfectly well for any list-like\n",
        "    Python object (generally, anything having a <code>next()</code> method will\n",
        "    function as well). See the next subsection for an explanation of this\n",
        "    function. \n",
        "\n",
        "*   <code>split_yielder</code> calls <code>data_yielder</code>, collects all\n",
        "    the elements from <code>data_yielder</code>, then yields tuples \n",
        "\n",
        "    <code>(subject number excluded, training_split_data, testing_split_data)</code>\n",
        "\n",
        "    where the data from subject number <code>subject number excluded</code> \n",
        "    is <code>testing_split_data</code> and everyone else's data is stacked\n",
        "    into <code>training_split_data</code>. If <code>t</code> is one of these\n",
        "    splits, then it is a tuple with elements:\n",
        "    \n",
        "    *   <code>t[0]</code> is the model input data, the structure \n",
        "    depending on the <code>with_time</code> parameter. If \n",
        "    <code>with_time=True</code> (the default) \n",
        "    this is a tuple of length 2: <code>t[0][0]</code> are the spectrograms\n",
        "    as a numpy array with shape (*, 65, 73) and <code>t[0][1]</code> are the\n",
        "    timestamps, as a numpy array with shape (*,1) or possibly (*,). If \n",
        "    <code>with_time=False</code> then <code>t[0]</code> is just the \n",
        "    array of spectrograms.\n",
        "    \n",
        "    *   <code>t[1]</code> is the numpy array of PSG labels, having values in\n",
        "    <code>[0, 1, 2, 3, 4, 5]</code> with 0 indicating awake and 5 indicating\n",
        "    REM. Thus, you likely need to process <code>t[1]</code> more using \n",
        "    <code>np.where</code> or <code>np.piecewise</code> to labels you \n",
        "    want. For sleep-wake labeling, try  \n",
        "        <code>psg_sleep_wake = np.where(t[1] > 0, 1, 0)</code>  \n",
        "\n",
        "The data pickles being yielded above are stored in the <code>pickles</code> folder inside the Drive folder pointed to by <code>data_path</code>, defined below. The data for subject <code>123</code> is stored in the folder <code>data_path/pickles/123</code>; there you will find three files:\n",
        "\n",
        "\n",
        "1.   <code>psg.pickle</code>\n",
        "2.   <code>spectros.pickle</code>\n",
        "3.   <code>time.pickle</code>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtsnGuFRfMkU",
        "colab_type": "text"
      },
      "source": [
        "### 0.2 Explanation of the pickles\n",
        "\n",
        "Assume we have loaded these three pickles for subject <code>123</code> into the variables <code>psg, spectros,</code> and <code>time</code>. These are all numpy arrays whose axis-0 lengths are the same.  \n",
        "*   <code>psg[i]</code> is the PSG label for the time window <code>[30\\*i, 30\\*(i+1)]</code>. \n",
        "*   <code>time[i]</code> is equal to <code>30\\*i + 15</code>, the center of this time window, and \n",
        "*    <code>spectros'[i]</code> is the spectrogram of derivative of magnitude of \n",
        "    acceleration (quite a mouth full...) for times ranging between \n",
        "    <code>30\\*i + 15 - 46.08</code> and <code>30\\*i + 15 + 46.08</code>. \n",
        "    The somewhat strange number <code>46.08 = 92.16/2</code> is a consequence of wanting 90 second windows with data sampling rate of 50Hz, but also wanting the number of records in this window (a priori, 90/0.02 = 4500) to be divisble by 128, the number of points per Fourier transform used in the spectrogram creation. So, the shortest time interval over 90 seconds satisfying the additional divisibility requirement is 92.16 seconds, since 4500 = 20 mod 128, or 4608 = 4500 + (128 - 20). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uNCbeOhef6L",
        "colab_type": "text"
      },
      "source": [
        "### 0.3 Olivia: Adding circadian predictions\n",
        "*Olivia: you should save the circadian values for each window in this folder, too. Probably easiest to modify <code>etl.py</code> adding a step to load and then save the circadian value in a separate pickle. Or, you could add a column to each time.pickle, giving that array shape (?, 2) instead of (?, 1). I'll indicate in the cell below defining our CNN how to modify things in either case.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvL-FHTmgLFg",
        "colab_type": "text"
      },
      "source": [
        "## 1. Preamble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMlFvvNo3j0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import precision_recall_curve as pr_curve\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import concatenate, Conv2D, Dense, Dropout, Flatten, Input, MaxPool2D \n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "import pickle\n",
        "import glob\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfFdf4GrkXRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See if a GPU is available. \n",
        "# This speeds up training the NNs, with significant gains on the CNN.\n",
        "gpu = tf.test.gpu_device_name()\n",
        "if \"GPU\" in gpu:\n",
        "    print(gpu)\n",
        "else:\n",
        "    print(\"No GPU found!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kW3k2AQo8qh",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Define paths for finding data and support libraries\n",
        "These paths should be from a mounted Google Drive. You should change <code>data_path</code> to point to the directory where your copies of the pickled spectrograms/PSG labels are stored, and "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ypTqd_C7k8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = \"/content/drive/My Drive/sleep_classifiers_accel_ML/data/\"\n",
        "source_path = \"/content/drive/My Drive/sleep_classifiers_accel_ML/source/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG-52gSrjAt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path.append(source_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQowEg05gufz",
        "colab_type": "text"
      },
      "source": [
        "The following library should be stored as \n",
        "<code>source_path + \"evaluator.py\"<c/code> and is obtainable from <a href=\"https://github.com/ericcanton/sleep_classifiers_accel_ML/blob/master/source/evaluator.py\">Eric's GitHub</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzjpT9CDjDdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import evaluator as ev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGIt861RqDDz",
        "colab_type": "text"
      },
      "source": [
        "## 2. CNN Definitions\n",
        "We define two CNNs, one for sleep/wake classification, the second for staging. These both incorporate time, and can easily be modified to incorporate further inputs. I have detailed how one would do this in model 2.1 immediately below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF7WwGzlkN2-",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 CNN for predictiong sleep/wake using time\n",
        "The last layer has 2 neurons because we want to predict 2 classes: \n",
        "*   0 for \"awake\"\n",
        "*   1 for \"asleep\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iTv2-lLBica",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##\n",
        "# Model input layers\n",
        "# The shape parameters do not include the number of training samples, \n",
        "# which should be the length of axis-0 of the input numpy tensors. \n",
        "##\n",
        "\n",
        "# Spectrograms. These must be reshaped using .reshape(-1, 65, 73, 1)\n",
        "# from their original shape (-1, 65, 73) to make them appear to be \"black and white images\"\n",
        "# to the Conv2D layers below. \n",
        "inputs_spectro = Input(shape=(65, 73, 1)) \n",
        "inputs_time = Input(shape=(1,))\n",
        "#####\n",
        "#####\n",
        "# Add more layers here. For example, if we wanted to add\n",
        "# the original x,y,z accleration data (averaged over the time window, e.g.)\n",
        "# then we would add\n",
        "#   inputs_accel = Input(shape(3,))\n",
        "#####\n",
        "#####\n",
        "\n",
        "# Convolutional layers. Operate only on spectrograms\n",
        "c_1 = Conv2D(64, kernel_size=3, activation='relu', data_format='channels_last')(inputs_spectro)\n",
        "m_1 = MaxPool2D()(c_1)\n",
        "c_2 = Conv2D(32, kernel_size=3, activation='relu', data_format='channels_last')(m_1)\n",
        "m_2 = MaxPool2D()(c_2)\n",
        "\n",
        "# The output of m_2 is a stream of two-dimensional features\n",
        "# Densely connected layers want one-dimensional features, so we \"unwind\"\n",
        "# the two-dimensional features using a Flatten layer:\n",
        "#   ***\n",
        "#   ***\n",
        "#   ***\n",
        "#   *** ~~~> ***,***,***,***\n",
        "flat = Flatten(data_format='channels_last')(m_2)\n",
        "\n",
        "# Now append time to the front of this flattened data\n",
        "concat = concatenate([ \\\n",
        "#####\n",
        "#####\n",
        "# If you added layers above, include them in the list inside concatenate.\n",
        "# Order shouldn't really matter, just add them below. \n",
        "#####\n",
        "#####\n",
        "                      inputs_time, \\\n",
        "                      flat])\n",
        "    \n",
        "# Start of densely connected layers\n",
        "d_1 = Dense(32, activation='relu')(concat)\n",
        "drop = Dropout(0.05)(d_1)\n",
        "d_2 = Dense(32, activation='relu')(drop)\n",
        "d_3 = Dense(32, activation='relu')(d_2)\n",
        "\n",
        "# Two categories predicted, giving 2 neurons. activation='softmax' makes\n",
        "# these two nodes sum to 1 and be bounded between 0 and 1.\n",
        "out = Dense(2, activation='softmax')(d_3)\n",
        "\n",
        "cnn_time = Model(inputs=[inputs_spectro, inputs_time], outputs=out)\n",
        "cnn_time.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxwekwJLnGGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save this trained model to the attached Google drive. Since the weights in\n",
        "# each layer of the model are randomly initialized, this is important to ensure\n",
        "# we're starting with exactly the same pre-training state to compare performance\n",
        "# of *this exact model instance* on the training splits.\n",
        "cnn_time.save(source_path + \"cnn_model_binary_softmax.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE6admUckV1j",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 CNN for W/N12/N34/REM staging\n",
        "The model below is identical to the one in 2.1 aside from there being 4 neurons\n",
        "on the last Dense layer corresponding to the four stages of sleep above. These \n",
        "stages MUST be <code>[0,1,2,3]</code> in the PSG labeling for the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vdIG1WzsUUxM",
        "colab": {}
      },
      "source": [
        "# Model input layers\n",
        "inputs_spectro = Input(shape=(65, 73, 1))\n",
        "inputs_time = Input(shape=(1,))\n",
        "\n",
        "# Convolutional layers. Operate only on spectrograms\n",
        "c_1 = Conv2D(64, kernel_size=3, activation='relu', data_format='channels_last')(inputs_spectro)\n",
        "m_1 = MaxPool2D()(c_1)\n",
        "c_2 = Conv2D(32, kernel_size=3, activation='relu', data_format='channels_last')(m_1)\n",
        "m_2 = MaxPool2D()(c_2)\n",
        "\n",
        "# Flatten the layer to prepare for the densely connected layers\n",
        "flat = Flatten(data_format='channels_last')(m_2)\n",
        "# Now append time to the front of this flattened data\n",
        "concat = concatenate([inputs_time, flat])\n",
        "    \n",
        "# Start of densely connected layers\n",
        "d_1 = Dense(32, activation='relu')(concat)\n",
        "drop = Dropout(0.05)(d_1)\n",
        "d_2 = Dense(32, activation='relu')(drop)\n",
        "d_3 = Dense(32, activation='relu')(d_2)\n",
        "\n",
        "# Two categories predicted\n",
        "out = Dense(4, activation='softmax')(d_3)\n",
        "\n",
        "cnn_time = Model(inputs=[inputs_spectro, inputs_time], outputs=out)\n",
        "cnn_time.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bkLuwo2Fcs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_time.save(source_path + \"cnn_model_staging_softmax.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT1cQg51OKD3",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 No time\n",
        "Same as the first, but without time and so no concatenate layer between Flatten and Dense. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRRTFr2o7246",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model input layers\n",
        "inputs_spectro = Input(shape=(65, 73, 1))\n",
        "\n",
        "# Convolutional layers. Operate only on spectrograms\n",
        "c_1 = Conv2D(64, kernel_size=3, activation='relu', data_format='channels_last')(inputs_spectro)\n",
        "m_1 = MaxPool2D()(c_1)\n",
        "c_2 = Conv2D(32, kernel_size=3, activation='relu', data_format='channels_last')(m_1)\n",
        "m_2 = MaxPool2D()(c_2)\n",
        "\n",
        "# Flatten the layer to prepare for the densely connected layers\n",
        "flat = Flatten(data_format='channels_last')(m_2)\n",
        "    \n",
        "# Start of densely connected layers\n",
        "d_1 = Dense(32, activation='relu')(flat)\n",
        "drop = Dropout(0.05)(d_1)\n",
        "d_2 = Dense(32, activation='relu')(drop)\n",
        "d_3 = Dense(32, activation='relu')(d_2)\n",
        "\n",
        "# Two categories predicted\n",
        "out = Dense(2, activation='softmax')(d_3)\n",
        "\n",
        "cnn_no_time = Model(inputs=inputs_spectro, outputs=out)\n",
        "cnn_no_time.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9kfpt2o8XW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_no_time.save(source_path + \"cnn_no_time_model_softmax.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoxberDBOHhb",
        "colab_type": "text"
      },
      "source": [
        "## 2. Densely connected network definition\n",
        "To make somewhat comparable capacity between models we replace the Conv2D\n",
        "layers in the above models with Dense layers having the same number of neurons.\n",
        "Basically, the expressive power of the network; too much capacity gives better training results but overfits. There is probably more hyperparameter work to be\n",
        "done here to assess the correct capacity for the DNN; how much is needed to \n",
        "have similar performance as CNNs? Mathematically, this is possible, but \n",
        "the width and depth may be exceedingly large. \n",
        "\n",
        "See <a href=\"http://www2.math.technion.ac.il/~pinkus/papers/neural.pdf\">Leshno, et al. (1993)</a>  \n",
        " *Multilayer Feedforward Networks with a Nonpolynomial Activation Function can Approximate Any Function* (Neural Networks, Vol 6, pp 861--867)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx4hsXAIOVPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input layers\n",
        "inputs_spectro = Input(shape=(65, 73))\n",
        "inputs_time = Input(shape=(1,))\n",
        "\n",
        "# Flatten the spectrogram input\n",
        "flat = Flatten()(inputs_spectro)\n",
        "\n",
        "# Pass spectrograms through several densely connected layers before incorporating time.\n",
        "d_1 = Dense(64, activation='relu')(flat)\n",
        "#drop_1 = Dropout(0.2)(d_1)\n",
        "d_2 = Dense(32, activation='relu')(d_1)\n",
        "#m_2 = MaxPool1D()(d_2)\n",
        "\n",
        "# Now append time to the front of this flattened data\n",
        "concat = concatenate([inputs_time, d_2])\n",
        "    \n",
        "# Start of densely connected layers\n",
        "d_3 = Dense(32, activation='relu')(concat)\n",
        "drop = Dropout(0.05)(d_3)\n",
        "d_4 = Dense(32, activation='relu')(drop)\n",
        "d_5 = Dense(32, activation='relu')(d_4)\n",
        "\n",
        "# Two categories predicted\n",
        "out = Dense(2, activation='softmax')(d_3)\n",
        "\n",
        "dnn_time = Model(inputs=[inputs_spectro, inputs_time], outputs=out)\n",
        "dnn_time.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1lnmRzaOcyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dnn_time.save(source_path + \"dnn_with_time.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70NA3r_QnA14",
        "colab_type": "text"
      },
      "source": [
        "## 3. Train networks on leave-one-out splits\n",
        "The loops below use <code>ev.split_yielder</code> to train the above-defined CNNs and DNNs on the training data, per the description of this function in section 0.1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owLWHhJPvDvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These describe the main part of the NN naming scheme for saving the trained neural networks.\n",
        "# For example, the CNN for sleep/wake classification would be trained on the training split excluding\n",
        "# subject 123, then saved to data_path + \"pickles/123/123_softmax_time_cnn.h5\"\n",
        "cnn_time_base = \"_softmax_time_cnn.h5\"\n",
        "cnn_no_time_base = \"_softmax_time_no_cnn.h5\"\n",
        "\n",
        "dnn_time_base = \"_softmax_time_dnn.h5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqmhrce-jL_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "for split in ev.split_yielder(data_path=data_path, exclude = ['7749105']):\n",
        "    # split == (subject in testing_split_data, training_split_data, testing_split_data)\n",
        "    # split[1] == 2-tuple with element [i]...\n",
        "    #   [0] : [training spectrograms, training time]\n",
        "    #   [1] : training psg\n",
        "    # Same elements for split[2], the testing data\n",
        "    train_spectros = split[1][0][0].reshape(-1, 65, 73, 1)\n",
        "    train_time = split[1][0][1].reshape(-1,1)\n",
        "    train_X = [train_spectros, train_time]\n",
        "    train_psg = np.where(split[1][1] > 0, 1, 0)\n",
        "\n",
        "    test_spectros = split[2][0][0].reshape(-1, 65, 73, 1)\n",
        "    test_time = split[2][0][1].reshape(-1,1)\n",
        "    test_X = [test_spectros, test_time]\n",
        "    test_psg = np.where(split[2][1] > 0, 1, 0)\n",
        "\n",
        "    count += 1 # Keeps track of progress through the training regime\n",
        "    print(\"Working on %s (%d of 28)\" % (split[0], count))\n",
        "\n",
        "    split_cnn_time = tf.keras.models.load_model(source_path + \"cnn_model_softmax.h5\")\n",
        "    split_cnn_no_time = tf.keras.models.load_model(source_path + \"cnn_no_time_model_softmax.h5\")\n",
        "    print(\"Neural networks loaded.\")\n",
        "\n",
        "    with tf.device(gpu):\n",
        "        split_cnn_time.fit(train_X, train_psg, epochs = 15, verbose = 1, validation_data = (test_X, test_psg))\n",
        "        split_cnn_no_time.fit(train_spectros, train_psg, epochs = 15, verbose = 1, validation_data = (test_spectros, test_psg))\n",
        "    split_cnn_time.save(data_path + \"neural/\" + split[0] + cnn_time_base)\n",
        "    split_cnn_no_time.save(data_path + \"neural/\" + split[0] + cnn_no_time_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kEgljl28GeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "for split in ev.split_yielder(data_path=data_path, exclude = ['7749105']):\n",
        "    # split == (subject in testing_split_data, training_split_data, testing_split_data)\n",
        "    # split[1] == 2-tuple with element [i]...\n",
        "    #   [0] : [training spectrograms, training time]\n",
        "    #   [1] : training psg\n",
        "    # Same elements for split[2], the testing data\n",
        "    train_spectros = split[1][0][0]\n",
        "    train_time = split[1][0][1].reshape(-1,1)\n",
        "    train_X = [train_spectros, train_time]\n",
        "    train_psg = np.where(split[1][1] > 0, 1, 0)\n",
        "\n",
        "    test_spectros = split[2][0][0]\n",
        "    test_time = split[2][0][1].reshape(-1,1)\n",
        "    test_X = [test_spectros, test_time]\n",
        "    test_psg = np.where(split[2][1] > 0, 1, 0)\n",
        "\n",
        "    count += 1 # Keeps track of progress through the training regime\n",
        "    print(\"Working on %s (%d of 28)\" % (split[0], count))\n",
        "\n",
        "    split_dnn_time = tf.keras.models.load_model(source_path + \"dnn_with_time.h5\")\n",
        "    print(\"Neural networks loaded.\")\n",
        "\n",
        "    split_dnn_time.fit(train_X, train_psg, epochs = 15, verbose = 1, validation_data = (test_X, test_psg))\n",
        "    split_dnn_time.save(data_path + \"neural/\" + split[0] + dnn_time_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRqgPggssPjV",
        "colab_type": "text"
      },
      "source": [
        "## 4. Evaluate\n",
        "We use the <code>pr_roc_from_path</code> function defined in <code>evaluator.py</code> to loop over the trained neural networks with the left-out subject's data as testing data. Passing <code>saveto=data_path + \"abc.png\"</code> will write the generated PR or ROC curve as abc.png, saved in the folder pointed to by <code>data_path</code>. If <code>saveto=None</code> (default) no picture is saved, just displayed below the cell.\n",
        "\n",
        "Options for <code>mode</code> are <code>\"roc\"</code> (the default) and <code>\"pr\"</code> that determine the obvious evaluation plot type. The output ROC or PR curve has many orangish-yellow curves and one blue curve. The blue curve is the pointwise mean of the other curves. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nejg8i6Btght",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ev.pr_roc_from_path(\n",
        "    ev.data_yielder(data_path, exclude = ['7749105'], nn_base_name =cnn_time_base), \\\n",
        "    title=\"Leave-one-out training splits using time: ROC curve for CNN\", \\\n",
        "    # Change this to whatever the class label is you want considered \"positive\" (default: 0 = sleep) \\\n",
        "    pos_label=0, \\\n",
        "    # used for y/x axis labeling. If None, defaults to \"True/False positive rate\"\n",
        "    label_names = {'positive' : \"Awake\", 'negative' : \"Sleep\"}, \\\n",
        "    # Do not save the picture. Change this a string with full path of image file you want to write\n",
        "    saveto=None, \\\n",
        "    # make ROC plot instead of PR plot with =\"pr\"\n",
        "    mode=\"roc\", \\\n",
        "    # we build our NNs with softmax activation at the final layer\n",
        "    from_logits=False) \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsOihIN7wnXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}